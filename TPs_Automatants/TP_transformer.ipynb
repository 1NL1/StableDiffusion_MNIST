{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5YU4mWXLYqGL"
      },
      "source": [
        "# Transformer\n",
        "Bienvenue à l'un des TPs les plus longs de ce repo. Aujourd'hui on va s'intéresser à l'architecture des transformers.\n",
        "Un transformer est un réseau de neurones qui permet de transformer des séquences de données en d'autres séquences de données. Il est très utilisé dans le domaine du NLP pour faire de la traduction de texte par exemple. C'est aussi l'architecture derrière le fameux ChatGPT, que vous connaissez déjà.\n",
        "\n",
        "Les modèles de génération d'images comme DALLE utilisent aussi des transformers, pour encoder le texte.\n",
        "\n",
        "![chatgpt](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/chatgpt.png)\n",
        "\n",
        "![dalle](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/dalle.png)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Personnellement, je trouve que c'est plus facile à comprendre les transformers en commençant par comprendre comment l'utiliser (comprendre les entrées et les sorties) et ensuite en regardant l'architecture.\n",
        "On va donc commencer par un exemple d'utilisation, puis on va rentrer dans le détail de l'architecture.\n",
        "Donc pour l'instant, on va considérer le transformer comme un black box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YY-ixtD3YqGM"
      },
      "source": [
        "\n",
        "## Inférence (ou la phase d'utilisation/prédiction)\n",
        "\n",
        "Pendant l'inférence, le transformer prédit un token (ou un mot, ou un élément de la séquence, c'est la même chose) à la fois. Il prend en compte les tokens précédemment prédits et la séquence d'origine comme entrées pour prédire le token suivant.\n",
        "\n",
        "Considérons l'exemple de la traduction : le transformer utilise à la fois la phrase source et les mots précédemment prédits de la phrase traduite comme entrées afin de générer le mot suivant de la phrase traduite.\n",
        "\n",
        "![blackbox](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/blackbox.png)\n",
        "\n",
        "Pendant la traduction, le transformer prend en entrée la phrase source et la phrase actuellement prédite, puis génère le mot suivant dans la phrase traduite en se basant sur ces deux entrées. Il répète ce processus, en prenant les mots précédemment prédits et la phrase source en entrée à chaque étape, jusqu'à ce que la phrase traduite complète soit générée.\n",
        "\n",
        "Examinons ça avec un exemple détaillé.\n",
        "\n",
        "Supposons que le transformer soit déjà entraîné et qu'on veut qu'il traduise \"I love oranges.\" en français. La phrase traduite qu'on veut obtenir est \"J'aime les oranges\".\n",
        "\n",
        "La phrase source est `[\"I\", \"love\", \"oranges\", \".\"]`.\n",
        "\n",
        "Comme c'est le début de l'inférence, la phrase actuellement prédite est vide, mais on ne peut pas mettre une phrase vide dans le transformer, donc on commence par `[\"<start>\"]` comme phrase actuellement prédite.\n",
        "\n",
        "On a donc ces deux séquences en entrées: `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\"]`.\n",
        "\n",
        "On va tokenizer ensuite ces séquences (convertir chaque mot en un nombre le représentant), par exemple `[\"I\", \"love\", \"oranges\", \".\"]` -> `[10, 35, 20, 49]` et `[\"<start>\"]` -> `[40]`.\n",
        "\n",
        "Le transformer prend ensuite en entrée ces deux séquences, puis génère un vecteur de probabilité sur l'ensemble du vocabulaire, représentant la probabilité de chaque mot possible suivant le token `<start>`.\n",
        "\n",
        "Puisque le transformer est déjà entraîné, le mot avec la probabilité la plus élevée va être `J'`. On ajoute `J'` à la phrase actuellement prédite.\n",
        "\n",
        "![first inference](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/probability1.png)\n",
        "\n",
        "On a donc maintenant `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\", \"J'\"]`. Avec la tokenisation, on a `[10, 35, 20, 49]` et `[40, 20]`, en supposant que 20 représente `J'`.\n",
        "Remarque qu'on a deux tokeniseurs différents pour l'anglais et le français.\n",
        "\n",
        "Le transformer prend ensuite en entrée ces séquences, puis génère deux vecteurs de probabilité, la première représentant la probabilité du mot possible suivant le token `<start>`, et la seconde celle du mot possible suivant le token `J'`.\n",
        "\n",
        "![second inference](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/probability2.png)\n",
        "\n",
        "Remarque que le premier vecteur de probabilité est identique au précédent. C'est parce que le transformer génère toujours les même vecteurs pour les mêmes entrées même s'il y a des tokens supplémentaires dans la phrase actuellement prédite. Les futurs tokens n'influencent pas les vecteurs de probabilité des tokens précédents. C'est parce qu'il y a un masque qu'on appelle `causal mask` qui empêche les tokens de prêter attention aux futurs tokens; on en discutera plus tard dans la section sur l'architecture du transformer.\n",
        "\n",
        "Puisque on a déjà prédit le mot suivant le token `<start>` à la première étape, on peut ignorer le premier vecteur de probabilité et se concentrer uniquement sur le second.\n",
        "\n",
        "\n",
        "Comme le transformer est déjà entraîné, le mot avec la probabilité la plus élevée va être \"aime\".\n",
        "\n",
        "On obtient donc `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\", \"J'\", \"aime\"]`.\n",
        "\n",
        "On répète ce processus jusqu'à ce que le transformer génère le token `<end>`, indiquant que la séquence générée est complète. Le transformer est entraîné pour prédire le token `<end>` de manière appropriée pendant la phase d'entraînement, ce qui nous permet de déterminer quand arrêter la boucle de génération."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hXKvFK_rYqGN"
      },
      "source": [
        "### Exercice 1: Utilisation d'un transformer pré-entraîné pour faire de la traduction de texte\n",
        "On va importer un modèle de transformer déjà entraîné et vous allez devoir faire la boucle d'inférence pour traduire la phrase \"I love oranges.\" en français.\n",
        "\n",
        "Téléchargement des libraries: `pip install transformers sentencepiece`.\n",
        "Si vous êtes sur colab, alors lancez cette cellule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9MI6P43YqGN",
        "outputId": "9af3659c-76d4-470b-afbe-b43885d5584b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xHnxh5-PYqGN"
      },
      "source": [
        "Maintenant on va load le modèle. Vous n'avez pas besoin de comprendre la cellule suivante, tout ce qu'il faut retenir c'est que vous avez un objet `transformer` qui contient le modèle et le tokenizer que vous allez utiliser pour faire l'inférence.\n",
        "\n",
        "Je vais vous expliquer comment utiliser le modèle et le tokenizer juste après."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "2eeb9012c8284105982551ffb1ea36c2",
            "13cca85f8a1d407d8205d40fa5d7865d",
            "db43aeb879cf498a9c268cd9acd7cb2e",
            "e63a7d5021f2469e8bc2e7995cbe4610",
            "38101c5ae77c4474970b614ed6a7fc48",
            "6976d6f37aaf452d99e968b173593313",
            "7540e7d3c6b445469cfae14e2a598700",
            "6d39b1d1ab5f4f9c9cc85e460f02bf2d",
            "28bcb4b2e8e64c89a5622d86a5e50404",
            "c9ac13ab3b8e495989138a6da945bbdf",
            "80646316a81346db98cfaaefeeb7e152",
            "1daf40c00c6f4066beb777cdba90d87f",
            "ce423e42d50a4c80beff2f7863c96020",
            "7fa78bfba6ff4c54ad8366da40c31b25",
            "a9061aa455af4b8e880025bf270a4839",
            "7e5bd1a5a1e54a8b9aaa1c1fb9b45f3e",
            "8309e4fbdf954bf583bec21662b1ddb7",
            "eded5eb555d14f24ba24b896cc088f6f",
            "91f60ae75393443db79d3f0ff9f37fe0",
            "51b79c8e04c742408de64eee64df7e0a",
            "6ac12a8d4107416bb4f07fc39303f7e8",
            "5eef299efee74f55818c9b921e1f78bf",
            "a11e93067160472abb3309e8322b36b0",
            "d275b399ed034fa6b5a60b7ad65ad9bf",
            "bf3dba6036dd4d21bdc5bccb450b45b3",
            "8fac82ffccbc41529671ee0e806f8027",
            "ffd56fde265f49eeba5291f27042d4e5",
            "27079775ba7b4a4a9347ec7b9d768312",
            "51c1ead0511e45e2a22ea0a2b6ed1f0e",
            "af71551cbae5415fbf719298776d1f0b",
            "7bf7b764ada746b6b1fbe49b5c1bc096",
            "4395abed933944f7ba93946050ef1c44",
            "7b867a5e78e14909b094fb61d2d608ec",
            "e083636fe4dd4bfdb0178ee3934ba88c",
            "57086f0faf014b09a15f94a57d359fa3",
            "3bb563b857b74c4bbe06b00253d55589",
            "162dbb76edfa4496810441673067d854",
            "f233bf96fb3f4a0c8475b23bbcf4b09f",
            "ea4078c832b243cdb885ed1dc5713cfc",
            "cbe85e322bc74ee297a01fabc2942757",
            "3daada782f16478b8b27bbd612d7b940",
            "696699949c9b4e32ae92dbd45db0172b",
            "57489af57ceb4874b34561e3b8d06122",
            "b16edc6321384038aec3c06faa5d5889",
            "2c21b85ad6484b2a91ae238ded11bdce",
            "e7ec809e228d4a679e195d9c031c1593",
            "886d5aee9ee348ba9e3cd4fb78bf512b",
            "b1ee4cc1f3784737981a120987132932",
            "311eda22f4b0427a9a2c654fc6c77021",
            "9ab8a35924a94da4b62d01f112c5d2cf",
            "d46ffc4ea91c464badc7f9593aedda29",
            "db489a0e84754ebbae1319228e5faa01",
            "718b1e05205e4669a0ddeb424fe9e23e",
            "18462d5d43754844b8911c95de0a1f67",
            "7626fdc1576c48d0a022b59b4ad9017d",
            "c5bbde62e74d435b94f74c12544cdd05",
            "a4e74f579726485e973b065ab9af4183",
            "7e8ce644c6584ccba81715cff1bb05e3",
            "1aab2d6b84da4448ba3689e13bf4fb9a",
            "9fcb9e3facfb4dc1a01e1707487e4783",
            "cf1cf9d261fa4809873795e2b9ecddd6",
            "ce0db6d8a522414caab25e054045b88f",
            "5ac734c1448240bcaf2fa54dea65ab0c",
            "043d91cd18f34986bf7ab0ae3d44ba6a",
            "01ca83a566b94e5f96ad7eef77787358",
            "0ed4eed9349f4fc88ecf0140516711b1",
            "c3b71307827047bea0879b2f87c535f2",
            "d5d196d39286478ba319f4d7769e4921",
            "7e2e7d6729324f25a814214f2bb60d28",
            "b13684f2d34d4371805377d4698c8c36",
            "d470c4248a8b41e3bebf9f85d9c3b21e",
            "242b006a3fbe4ac69234f0315be77a56",
            "d31de5cffe26477cafcf65d29edbf797",
            "5da83500038645f98bcc5db02f14b932",
            "48e2d52c1c514cb2b3976a3a23b6ca8c",
            "745ee10439254bf89b90163c408fc5ce",
            "ea8cfd4b592f494997032a607d203dda"
          ]
        },
        "id": "WcRbKEUGYqGO",
        "outputId": "42f707a2-a5d7-4bf1-f29c-88d730b189c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eeb9012c8284105982551ffb1ea36c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1daf40c00c6f4066beb777cdba90d87f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a11e93067160472abb3309e8322b36b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e083636fe4dd4bfdb0178ee3934ba88c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c21b85ad6484b2a91ae238ded11bdce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5bbde62e74d435b94f74c12544cdd05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3b71307827047bea0879b2f87c535f2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "class MyTransformer:\n",
        "    def __init__(self):\n",
        "        # Load pre-trained model and tokenizer\n",
        "        model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "        self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        self.model = MarianMTModel.from_pretrained(model_name)\n",
        "        self.start_token = self.model.config.decoder_start_token_id\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokenized_input = self.tokenizer(text, return_tensors='pt', truncation=True)\n",
        "        return tokenized_input[\"input_ids\"]\n",
        "\n",
        "    def detokenize(self, token_ids):\n",
        "        return [self.tokenizer._convert_id_to_token(token_id) for token_id in token_ids]\n",
        "\n",
        "    def forward(self, src_sequence, tgt_sequence):\n",
        "        \"\"\"\n",
        "        src_sequence: torch.LongTensor of shape (batch_size, seq_len) la séquence des ids des mots de la phrase source\n",
        "        tgt_sequence: torch.LongTensor of shape (batch_size, seq_len) la séquence des ids des mots de la phrase actuellement prédite\n",
        "\n",
        "        return: torch.FloatTensor of shape (batch_size, seq_len, vocab_size) la probabilité de chaque mot possible pour chaque token de la phrase actuellement prédite\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            encoder_states = self.model.get_encoder()(src_sequence)\n",
        "            decoder_states = self.model.get_decoder()(tgt_sequence, encoder_hidden_states=encoder_states.last_hidden_state)\n",
        "            prediction = self.model.lm_head(decoder_states.last_hidden_state)\n",
        "            prediction = torch.softmax(prediction, dim=-1)\n",
        "        return prediction\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "transformer = MyTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qx6SL3wVYqGO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bgPL6Ht0YqGO"
      },
      "source": [
        "Dans `transformer`, vous avez 2 méthodes que vous pouvez utiliser pour l'inférence: `transformer.tokenize` et `transformer.detokenize` pour convertir les phrases en nombres et vice versa.\n",
        "\n",
        "Lorsque vous faites ```transformer.tokenize(\"I love oranges.\")```, vous obtenez ```tensor([[47, 1779, 12610, 9, 3, 0]])```,\n",
        "\n",
        "Et lorsque vous faites ```transformer.detokenize([47, 1779, 12610, 9, 3, 0])```, vous obtenez ```['▁I', '▁love', '▁orange', 's', '.', '</s>']```.\n",
        "\n",
        "On remarque qu'il y a \"\\_\" devant certains mots. C'est parce que le tokenizer sépare certains mots en sous mots. Par exemple, \"love\" est devenu \"\\_love\", mais \"orange\" est séparé en \"\\_orange\" et \"s\". C'est juste une méthode avancée de faire de la tokenisation, et il y a d'autres façons de faire comme du mot par mot ou du caractère par caractère. ChatGPT par exemple utilise du \"Byte Pair Encoding\".\n",
        "Le \"\\_\" représente un espace, donc \"\\_love\" est \" love\" et \"s\" est juste \"s\".\n",
        "\n",
        "D'ailleurs, `</s>` est le token `<end>`.\n",
        "\n",
        "Pour utiliser le modèle pré entraîné, vous allez devoir faire `transformer(input_ids, decoder_input_ids)`, où `input_ids` est la séquence tokenisée de la phrase source et `decoder_input_ids` est la séquence tokenisée de la phrase actuellement prédite. `input_ids` et `decoder_input_ids` doivent être des `torch.tensor` de shape `(batch_size, seq_len)` et de type `long`.\n",
        "\n",
        "Par exemple `transformer.tokenize(\"I love oranges.\")` est un valide `input_ids` car il est de shape `(1, 6)` et de type `long`.\n",
        "\n",
        "`transformer(input_ids, decoder_input_ids)` va retourner un `torch.tensor` de shape `(batch_size, seq_len, vocab_size)` qui correspond aux vecteurs de probabilité de chaque token de la phrase actuellement prédite. `vocab_size` est le nombre de mots dans le vocabulaire du modèle. Donc pour obtenir le mot suivant, vous allez devoir trouver l'élément avec la plus grande probabilité dans le dernier vecteur de probabilité de `transformer(input_ids, decoder_input_ids)`.\n",
        "\n",
        "`transformer.start_token` est l'id du token `<start>`. Vous allez devoir l'utiliser pour initialiser `decoder_input_ids` à la première étape de la boucle d'inférence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7o3DOT8YqGP",
        "outputId": "d661c87e-31a1-461a-af70-cca25d1526f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   47,  1779, 12610,     9,     3,     0]])\n",
            "['▁I', '▁love', '▁orange', 's', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(transformer.tokenize(\"I love oranges.\"))\n",
        "print(transformer.detokenize([47, 1779, 12610, 9, 3, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uugvt6SeYqGP"
      },
      "source": [
        "Par exemple, si vous voulez prédire le premier mot de la phrase \"I love oranges.\", vous allez devoir faire:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOWSjNDTYqGP",
        "outputId": "bf5bb60d-2e88-41da-9e5a-afe14bdd8289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 59514])\n",
            "234\n",
            "['▁J']\n"
          ]
        }
      ],
      "source": [
        "input_ids = transformer.tokenize(\"I love oranges.\")\n",
        "decoder_input_ids = torch.tensor([[transformer.start_token]], dtype=torch.long)\n",
        "\n",
        "prediction = transformer(input_ids, decoder_input_ids)\n",
        "print(prediction.shape)\n",
        "\n",
        "id_next_word = torch.argmax(prediction[0, -1]).item()\n",
        "print(id_next_word)\n",
        "\n",
        "next_word = transformer.detokenize([id_next_word])\n",
        "print(next_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ByB-QXjAYqGP"
      },
      "source": [
        "### A vous de jouer\n",
        "Maintenant que vous savez comment utiliser le modèle, essayez de faire la boucle d'inférence pour traduire la phrase \"I love oranges.\" en français en utilisant le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXIc_pB1YqGQ",
        "outputId": "de03704a-519f-469f-f44c-3c3112e00327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 59514])\n",
            "234\n",
            "['▁J']\n",
            "tensor([[59513,   234]])\n",
            "torch.Size([1, 2, 59514])\n",
            "6\n",
            "[\"'\"]\n",
            "tensor([[59513,   234,     6]])\n",
            "torch.Size([1, 3, 59514])\n",
            "17711\n",
            "['adore']\n",
            "tensor([[59513,   234,     6, 17711]])\n",
            "torch.Size([1, 4, 59514])\n",
            "16\n",
            "['▁les']\n",
            "tensor([[59513,   234,     6, 17711,    16]])\n",
            "torch.Size([1, 5, 59514])\n",
            "12610\n",
            "['▁orange']\n",
            "tensor([[59513,   234,     6, 17711,    16, 12610]])\n",
            "torch.Size([1, 6, 59514])\n",
            "9\n",
            "['s']\n",
            "tensor([[59513,   234,     6, 17711,    16, 12610,     9]])\n",
            "torch.Size([1, 7, 59514])\n",
            "3\n",
            "['.']\n",
            "tensor([[59513,   234,     6, 17711,    16, 12610,     9,     3]])\n",
            "▁J'adore▁les▁oranges.\n"
          ]
        }
      ],
      "source": [
        "res = \"\"\n",
        "input_ids = transformer.tokenize(\"I love oranges.\")\n",
        "decoder_input_ids = torch.tensor([[transformer.start_token]], dtype=torch.long)\n",
        "\n",
        "for i in range(len(input_ids[0])+1):\n",
        "  prediction = transformer(input_ids, decoder_input_ids)\n",
        "  print(prediction.shape)\n",
        "\n",
        "  id_next_word = torch.argmax(prediction[0, -1]).item()\n",
        "  print(id_next_word)\n",
        "\n",
        "  next_word = transformer.detokenize([id_next_word])\n",
        "  print(next_word)\n",
        "  res += next_word[0]\n",
        "\n",
        "  decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[id_next_word]])], dim=1)\n",
        "  print(decoder_input_ids)\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lrSggat0YqGQ"
      },
      "source": [
        "Normalement, vous devriez obtenir \"J'adore les oranges.\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q1GrIOLdYqGQ"
      },
      "source": [
        "## Entraînement\n",
        "Avant de regarder l'architecture d'un transfo, on va regarder comment on entraîne un transformer. La phase d'entraînement est un peu différente de la phase d'inférence.\n",
        "En fait, pendant l'entraînement, au lieu de prédire un token à la fois, le transformer va prédire tous les tokens de la target sequence (la phrase qu'on doit prédire) en même temps.\n",
        "\n",
        "Par exemple, reprenons l'exemple \"I love oranges.\". La phrase qu'on veut obtenir est \"J'aime les oranges.\". Au lieu de prédire un mot à la fois, le transformer va prédire `[J', aime, les, oranges, ., <end>]` (enfin, il va essayer puisqu'il n'est pas encore entraîné).\n",
        "\n",
        "Plus précicément, on va donner au transformer les deux phrases `[I, love, oranges, .]` et `[<start>, J', aime, les, oranges, .]` en `input_ids` et `decoder_input_ids` respectivement, et on veut qu'il:\n",
        "- prédise `J'` à partir de `[<start>]`\n",
        "- prédise `aime` à partir de `[<start>, J']`\n",
        "- prédise `les` à partir de `[<start>, J', aime]`\n",
        "- prédise `oranges` à partir de `[<start>, J', aime, les]`\n",
        "- prédise `.` à partir de `[<start>, J', aime, les, oranges]`\n",
        "- prédise `<end>` à partir de `[<start>, J', aime, les, oranges, .]`\n",
        "\n",
        "Donc on veut qu'il fasse un peu la même chose qu'en inférence, mais tout parallèlemnt et en un coup, c'est-à-dire prédire directement tous les tokens `[J', aime, les, oranges, ., <end>]` en même temps.\n",
        "\n",
        "De plus, comme on donne `[<start>, J', aime, les, oranges, .]` à notre transformer, il faudra faire en sorte que le transformer ne puisse pas \"tricher\" en regardant directement les futurs tokens pour prédire le token actuel. On discutera de ça plus tard dans l'architecture du transformer.\n",
        "\n",
        "\n",
        "![training phase](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/prediction.png)\n",
        "\n",
        "Bref, donc notre target sequence (ou labels si vous voulez) est `[J', aime, les, oranges, ., <end>]`. Et on va devoir calculer la loss entre la prédiction du transformer et la target sequence.\n",
        "\n",
        "Voici un schéma qui illustre l'entraînement du transfo:\n",
        "\n",
        "![detailed training](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/prediction_6vectors.png)\n",
        "\n",
        "Remarque que comme la phrase actuellement prédite (ou `decoder_input_ids`, ou `inp_tgt` dans le schéma) contient 6 tokens, alors la prédiction du transformer (ou `out_tgt` dans le schéma) va contenir 6 vecteurs de probabilité aussi, et que notre target sequence (ou `tgt` dans le schéma) contient 6 tokens également.\n",
        "\n",
        "On va calculer la cross entropy loss entre chaque vecteur de probabilité de la prédiction du transformer et chaque token de la target sequence. Donc on va avoir 6 loss, et on va faire la moyenne de ces 6 loss pour avoir la loss finale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RPh2Udb0YqGQ"
      },
      "source": [
        "### Exercice: calculer la cross entropy loss du modèle `transformer` pour la phrase \"I love oranges.\" en français (\"J'aime les oranges.\")\n",
        "Sachant que `[234, 6, 17711, 16, 12610, 9, 3, 0]` correspond à la tokenisation de la séquence `[J, ', adore, les, orange, s, ., <end>]` en français.\n",
        "Calculer la moyenne des 6 cross entropy loss entre la prédiction du transformer et la target sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNAM9AbfYqGQ",
        "outputId": "3713b6f8-7186-4ce7-bcde-608e48305440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.5268)\n"
          ]
        }
      ],
      "source": [
        "target_ids = [234, 6, 17711, 16, 12610, 9, 3, 0]\n",
        "##target_ids_onehot = torch.nn.functional.one_hot(torch.tensor(target_ids), num_classes=transformer.model.config.vocab_size)\n",
        "prediction = transformer(input_ids, decoder_input_ids)\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(prediction[0], torch.tensor(target_ids))\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HSK_USrMYqGR"
      },
      "source": [
        "Normalement, vous devriez obtenir une loss de 10.526789665222168. Si vous obtenez une différence de plus de 0.01, ce n'est probablement pas normal, il faudra vérifier votre code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d93vXzOgYqGR"
      },
      "source": [
        "## L'architecture du transformer\n",
        "Maintenant que vous avez compris les entrées et les sorties du transformer, on va regarder l'architecture du transformer. On va construire le transformer petit à petit en python.\n",
        "\n",
        "![transformer architecture](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/architecture.png)\n",
        "\n",
        "Dans le transformer, deux séquences sont données en entrée: Inputs et Outputs (shifted right) (`input_ids` et `decoder_input_ids` dans notre cas).\n",
        "\n",
        "![input and output shifted right](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/inputs_archi.png)\n",
        "\n",
        "Dans le cas de la traduction, Inputs est la phrase à traduire, et Outputs (shifted right) est la phrase actuellement prédite.\n",
        "\n",
        "Par exemple, pour notre cas Inputs est `[I, love, oranges, .]` et Outputs (shifted right) est:\n",
        "- `[<start>, J', aime, les, oranges, .]` si on est dans la phase d'entraînement\n",
        "- `[<start>]` si on est au début de la phase d'inférence, et `[<start>, J']` si on a déjà prédit `J'` et qu'on veut prédire `aime` maintenant, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "byUsQ5rcYqGR"
      },
      "source": [
        "\n",
        "### Inputs embedding\n",
        "\n",
        "![inputs](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/input_emb_box.png)\n",
        "\n",
        "La couche inputs embedding sert juste à transformer les tokens en vecteurs de dimension `d_model`. Pour du traitement de texte, on utilise la plupart de temps la couche `nn.Embedding` que vous avez vu dans le TP sur l'introduction à NLP.\n",
        "\n",
        "Exemple:\n",
        "![embedding](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/exembedding.png)\n",
        "Ici, le token 10 est transformé en vecteur `[0.2, -0.5, ..., 1.2, -0.7]`.\n",
        "\n",
        "Les valeurs de ces vecteurs seront apprises pendant l'entraînement à l'aide de la backpropagation. Durant l'entraînement, le transformer va ajuster ces valeurs pour que les vecteurs représentent les features les plus utiles et informatives des tokens.\n",
        "\n",
        "Intuitivement, on pourrait imaginer que les mots synonymes auront des vecteurs similaires, et les mots antonymes auront des vecteurs opposés.\n",
        "\n",
        "\n",
        "#### Similarité\n",
        "\n",
        "Mais... que'est-ce que ça veut dire \"similaires\" et \"opposés\" dans ce contexte? Comment est-ce qu'on peut mesurer la similarité entre deux vecteurs?\n",
        "\n",
        "Deux méthodes les plus connues sont: la distance euclidienne et la similarité cosinus.\n",
        "\n",
        "En NLP, on utilise la plupart du temps la similarité cosinus (voire le produit scalaire pour gagner du temps de calcul).\n",
        "\n",
        "![cosine similarity](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/distance.png)\n",
        "\n",
        "Une question se pose alors: Pourquoi la similarité cosinus et pas distance euclidienne?\n",
        "\n",
        "Dans beaucoup de tâches de NLP, la direction dans laquelle un vecteur pointe (son orientation dans l'espace) est plus importante que sa longueur.\n",
        "La similarité cosinus et le produit scalaire donnent une mesure directe de la similarité en termes de direction de deux vecteurs alors que la distance euclidienne ne mesure que la distance entre deux vecteurs.\n",
        "\n",
        "Exemple:\n",
        "- La personne A a acheté 1x oeufs, 1x farine et 1x sucre.\n",
        "- La personne B a acheté 100x oeufs, 100x farine et 100x sucre\n",
        "- La personne C a acheté 1x oeufs, 1x Vodka et 1x Red Bull\n",
        "\n",
        "Par la similarité cosinus, A et B sont plus similaires.\n",
        "Par la distance euclidienne, A et C sont plus similaires.\n",
        "\n",
        "Et intuitivement, c'est plus logique de se dire que A et B sont plus similaires que A et C.\n",
        "\n",
        "Intuitivement, si deux mots sont synonymes, alors leur similarité cosinus sera proche de 1. Si deux mots sont antonymes, alors leur similarité cosinus sera proche de -1 et si deux mots sont complètement différents, alors leur similarité cosinus sera proche de 0, c'est-à-dire les deux vecteurs sont orthogonaux.\n",
        "\n",
        "#### Implémentation du transformer\n",
        "Donc pour l'instant, on a ça:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjJuVTT-YqGR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NyKgAtziYqGS"
      },
      "source": [
        "Notre modèle prend en entrée deux séquences de tokens: `input_ids` et `decoder_input_ids`. On passe `input_ids` (qui est de taille `(batch_size, seq_len)`) dans la couche `self.embedding` pour avoir `inputs` de taille `(batch_size, seq_len, d_model)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lALaITY_YqGS"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "![posenc](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/posenc_box.png)\n",
        "\n",
        "Le transformer traite tous les tokens en même temps, donc il n'y a pas de notion de position ou d'ordre dans la séquence.\n",
        "\n",
        "Pourquoi ça pose de problème?\n",
        "\n",
        "Exemple:\n",
        "- I love oranges and I hate apples.\n",
        "- I hate oranges and I love apples.\n",
        "\n",
        "Ces deux phrases n'ont pas le même sens, mais si on encode pas la position des tokens d'une manière ou d'une autre, pour le transformer, ces deux phrases sont identiques.\n",
        "\n",
        "L'idée va être de créer un vecteur de position pour chaque token, et d'additionner ces vecteurs de position aux vecteurs de token.\n",
        "\n",
        "![positional embeddings](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/posenc_add.png)\n",
        "\n",
        "On appelle ces vecteurs de position \"positional embeddings\".\n",
        "\n",
        "#### L'intuition derrière l'addition des positional embeddings\n",
        "Lorsqu'on additionne deux vecteurs, on déplace le vecteur de départ vers une direction donnée par le vecteur qu'on ajoute. Donc ici, en additionnant le positional embedding au token embedding, on déplace le token embedding vers le cluster/groupe des vecteurs qui représentent les tokens à cette position.\n",
        "\n",
        "![cluster](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/posenc_intuition.png)\n",
        "\n",
        "Par exemple, supposons que le premier mot de la phrase est \"car\". Le token embedding du mot \"car\" est localisé quelque part dans notre espace de `d_model` dimensions, et lorsque on additionne le positional embedding du premier mot, on déplace ce token embedding vers le cluster des mots qui sont à la première position.\n",
        "\n",
        "Ce processus va aider le modèle à différencier les mots qui sont à des positions différentes dans la séquence.\n",
        "\n",
        "#### Comment créer ces positional embeddings?\n",
        "Une méthode naïve va être de créer le vecteur `[1, 1, 1, ..., 1]` pour le positional embedding de la première position, `[2, 2, 2, ..., 2]` pour la deuxième position, etc.\n",
        "\n",
        "Mais cette méthode peut donner à des vecteurs de position de valeurs très grandes pour des phrases longues, qui dominent alors les valeurs des vecteurs de token embedding, ce qui rend difficile pour le modèle d'apprendre les features des tokens.\n",
        "\n",
        "On pourrait tenter de composer ces vecteurs par une fonction bornée comme le sigmoid, mais ça peut poser des problèmes car lorsque les valeurs sont trop grandes, le gradient du sigmoid devient très petit, et donc l'écart entre les vecteurs de position devient très petit, ce qui rend difficile pour le modèle de différencier les tokens à des positions différentes.\n",
        "\n",
        "![sigmoid](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/gradient_sigmoid.png)\n",
        "\n",
        "#### Solution des auteurs de l'article sur le transformer\n",
        "Dans le papier original sur le transformer, les auteurs ont proposé une méthode pour créer les positional embeddings qui est plus efficace que la méthode naïve.\n",
        "\n",
        "Imagse qui illustrent la méthode:\n",
        "\n",
        "![pe](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/pe.png)\n",
        "\n",
        "![posi](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/posi.png)\n",
        "\n",
        "$PE_{(pos, n)}$ est la valeur du positional embedding à la position $pos$ et à la dimension $n$.\n",
        "\n",
        "*We trust the author that it works lol* (un jour si j'ai la motivation je ferai une explication détaillée de l'intuition derrière cette méthode)\n",
        "\n",
        "Je vous file le code pour créer les positional embeddings car ce n'est pas la partie la plus intéressante du transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM-P3hIPYqGS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape = (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model) # shape = (max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        # maintenant, on veut faire self.pe = pe\n",
        "        # mais si on fait ça, lorsqu'on fait model.to(\"cuda\"), self.pe ne sera pas envoyé sur le GPU\n",
        "        # en utilisant register_buffer, on aura self.pe = pe et self.pe sera envoyé sur le GPU lorsqu'on fait model.to(\"cuda\")\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "        seq_len = x.shape[1]\n",
        "        x = x + self.pe[:seq_len].unsqueeze(0)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TmqiEwAcYqGS"
      },
      "source": [
        "Tout ce qu'il faut retenir c'est que la couche `PositionalEncoding` prend en entrée un tenseur de taille `(batch_size, seq_len, d_model)` (ça va être `inputs` pour notre cas) et retourne un tenseur de même taille, mais avec les positional embeddings additionnés aux token embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B68rYEYrYqGS"
      },
      "source": [
        "#### Exercice: Mettre à jour le code du transformer pour utiliser les positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3dFRxhqYqGT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        PE = self.positional_encoding(input_ids)\n",
        "        return inputs + PE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1IoVr-KHYqGT"
      },
      "source": [
        "#### Vérification de votre solution\n",
        "Retournez le résultat du forward, et lancer la cellule suivante. Vérifiez que vous obtenez `tensor([[[[0., 1.]]]])`. (ignorez le `grad_fn`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qnd7akkYqGT",
        "outputId": "571c40fb-eafb-4a30-b62f-1ba0e8bb98f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0000, 1.0000]]]], grad_fn=<SubBackward0>)\n"
          ]
        }
      ],
      "source": [
        "transfo = Transformer(1, 2)\n",
        "x = torch.tensor([[0]])\n",
        "print(transfo(x, None) - transfo.embedding(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1UW9wFZAYqGT"
      },
      "source": [
        "### Multi-head attention\n",
        "![mha](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/rectangle_mha_layer.png)\n",
        "\n",
        "### Single-head Self-Attention\n",
        "\n",
        "#### Intuition\n",
        "Considérons la phrase \"I love eating orange.\". Le mot \"orange\" a deux sens: la couleur, et le fruit.\n",
        "\n",
        "Notre but est alors de déterminer le sens du mot \"orange\" en fonction du contexte. Donc l'idée est de chercher des **key** words dans la phrase qui nous aideront à déterminer le sens du mot \"orange\" (le **query** word).\n",
        "Un moyen de faire ça est de calculer la similarité entre le query word et chaque mot de la phrase. Les mots qui ont une forte similarité avec le query word sont les mots qui nous aideront à déterminer le sens du query word.\n",
        "\n",
        "On va utiliser le produit scalaire pour calculer la similarité.\n",
        "\n",
        "![dot product](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/orange_dotproduct.png)\n",
        "\n",
        "Dans l'exemple illustré dans l'image, on voit que les mots \"orange\" et \"eating\" ont une forte similarité, avec un score de 1.2. Cela va nous aider à déterminer le sens du mot \"orange\" dans le contexte.\n",
        "On remarque d'ailleurs que le score de \"orange\" avec lui même est de 2, ce qui est normal car le mot \"orange\" est le query word, donc on s'attend à ce qu'il ait une forte similarité avec lui même.\n",
        "\n",
        "Bref, une fois qu'on a le score de similarité entre le query et chaque mot, on peut construire un vecteur qui représente le sens du query dans le contexte.\n",
        "Pour ça, on va faire une somme pondérée de tous les mots de la phrase, où les poids vont être les scores de similarité normalisés. Pour normaliser les scores, on va les passer par la fonction softmax.\n",
        "\n",
        "En faisant une somme pondérée, on accentue l'influence des mots du contexte qui ont des scores de similarité plus élevés, ce qui encapsule un sens sémantique plus précis pour le query, \"orange\".\n",
        "\n",
        "![softmax](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/3dvectorspace_orange.png)\n",
        "\n",
        "Intuitivement, ce processus va déplacer le vecteur du query word \"orange\" vers le cluster de mots qui ont un rapport avec les fruits.\n",
        "\n",
        "#### Détail\n",
        "\n",
        "Bon ça c'était l'intuition, maintenant on va voir plus en détail comment marche cette couche.\n",
        "\n",
        "La couche single-head self-attention prend en entrée une séquence de vecteurs générée par notre couche d'embedding et la couche positional encoding.\n",
        "\n",
        "Pour chaque vecteur $i$ de la séquence, trois vecteurs vont être générés: le vecteur **query**, **key**, et **value** en utilisant 3 couches linéaires. On les notera $Q_i$, $K_i$, $V_i$ respectivement.\n",
        "Ces vecteurs vont représenter différents aspects de la sémantique du mot dans le contexte de la phrase.\n",
        "\n",
        "![qkv](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/qkv.png)\n",
        "\n",
        "Maintenant, pour obtenir le sens contextuel d'un query word $Q_i$, on va calculer la similarité entre $Q_i$ et chaque mot de la séquence en faisant le produit scalaire entre $Q_i$ et chaque vecteur $K_j$.\n",
        "On obtient alors un vecteur de scores de similarité qu'on va normaliser en passant par la fonction softmax. On note le vecteur de scores normalisés $[\\alpha_1, ..., \\alpha_n]$.\n",
        "\n",
        "On peut enfin construire le nouveau vecteur qui représente le sens contextuel du mot $i$ en faisant la somme pondérée des vecteurs $V_j$ avec les poids $[\\alpha_1, ..., \\alpha_n]$.\n",
        "\n",
        "![softmax](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/qkv_dotprod.png)\n",
        "\n",
        "Essayons de comprendre pourquoi on utilise 3 vecteurs Q, K, V à la place d'un seul vecteur.\n",
        "\n",
        "Considérons la situation où le query word est \"network\". Dans ce contexte, on voudrait que les mots comme \"neural\" et \"social\" aient des scores de similarité élevés, car les phrases \"neural network\" et \"social network\" sont courantes. Ça veut dire que le produit scalaire entre le vecteur query et les vecteurs key pour ces mots devrait être élevé.\n",
        "\n",
        "Donc ça veut dire que les vecteurs keys associés à \"neural\" et \"social\" devraient être similaires. Mais comme \"neural\" et \"social\" veulent dire des choses différentes, leurs vecteurs values doivent différer considérablement. Si on utilisait un seul vecteur, cette différenciation ne serait pas possible.\n",
        "\n",
        "C'est pourquoi on utilise 3 vecteurs différents pour représenter chaque mots. Chaque vecteur représente un aspect différent de la sémantique du mot et sert à un but différent.\n",
        "\n",
        "Revenons à nos moutons. On a donc calculé le nouveau vecteur qui représente le sens contextuel du mot $i$.\n",
        "Bien sûr, on pourrait itérer ce processus pour chaque $i$ de la séquence, mais on peut aussi paralléliser et accélérer le calcul en utilisant les calculs matriciels pour obtenir tout en un coup.\n",
        "\n",
        "Notons $Q = [Q_1, ..., Q_n]$, $K = [K_1, ..., K_n]$, $V = [V_1, ..., V_n]$ les matrices qui contiennent les vecteurs query, key, et value de tous les mots de la séquence.\n",
        "\n",
        "Alors on peut obtenir les scores de similarité entre chaque query word et chaque mot de la séquence en faisant le produit matriciel $$\\text{score} = \\text{softmax}\\left(QK^T\\right)$$\n",
        "Puis on peut obtenir les nouveaux vecteurs en faisant $$V' = \\text{score} \\cdot V$$\n",
        "\n",
        "En réalité, avant de faire le softmax pour obtenir le score, on divise par un facteur $\\sqrt{d}$ où $d$ est la dimension des vecteurs query, key, et value. Les auteurs du papier ont dit que ça marche mieux donc on va les faire confiance.\n",
        "\n",
        "On obtient alors une formule compacte: $$V' = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\cdot V$$\n",
        "\n",
        "Illustration:\n",
        "![self attention](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/attention.png)\n",
        "\n",
        "Tout ce processus s'appelle le self-attention. C'est \"self\" parce que les query, key, et value sont tous des mots de la même séquence et c'est \"attention\" parce que le vecteur query \"fait attention\" aux vecteurs key pour déterminer le vecteur value.\n",
        "\n",
        "### Multi-head Self-Attention\n",
        "\n",
        "Dans la couche de single-head self-attention, on applique le processus d'attention une fois sur les données d'entrée. Pour l'attention multi-head, on applique ce processus plusieurs fois (d'où \"multi-head\"):\n",
        "Etant donné une séquence d'entrée de vecteurs de taille `(seq_len, d_model)` (si on prend pas en compte la dimension des batches), on divise `d_model` en $h$ parties, chacune de taille `d_model/h`. On obtient alors $h$ séquences de vecteurs de taille `(seq_len, d_model/h)`. On les appelle les *heads*.\n",
        "\n",
        "Pour chaque head, on a une séquence de vecteurs de taille `(seq_len, d_model/h)`, qui contient un sous-ensemble des features pour chaque mot de la phrase, et on peut appliquer le mécanisme de single-head self-attention à ce sous-ensemble de features.\n",
        "\n",
        "On obtient alors $h$ nouvelles séquences de vecteurs de taille `(seq_len, d_model/h)`.\n",
        "\n",
        "L'intuition ici est que chaque head peut se concentrer sur un aspect différent des données d'entrée. En divisant les features entre les heads, on permet à chaque head de se spécialiser dans certains types de relations.\n",
        "L'exemple suivante n'est pas trop réaliste, mais ça peut aider à construire l'intuition. Par exemple, un head pourrait capturer le genre (masculin, féminin, neutre) d'un nom tandis qu'un autre head pourrait capturer la cardinalité (singulier vs pluriel) d'un nom. Ça pourrait être important pendant la traduction parce que dans beaucoup de langues, le verbe qui doit être utilisé dépend de ces facteurs.\n",
        "\n",
        "Une fois qu'on a la sortie de chaque head, on doit recombiner ces sorties pour obtenir notre séquence finale de vecteurs, en concaténant les sorties de tous les heads le long de l'axe des features, ce qui donne un séquence finale de vecteurs de taille `(seq_len, d_model)`.\n",
        "\n",
        "Le split head est illustrée ci-dessous:\n",
        "![split head](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/splithead.png)\n",
        "\n",
        "Puis on passe chaque head dans une couche de single-head self-attention:\n",
        "![mha](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/multihead.png)\n",
        "\n",
        "![concat](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/concatenatehead.png)\n",
        "\n",
        "\n",
        "Ensuite, pour projeter la séquence finale dans notre espace de features original, on applique une couche linéaire finale. Les poids de cette couche sont appris pendant l'entraînement, ce qui permet au modèle de déterminer la meilleure façon d'intégrer l'information de tous les heads.\n",
        "\n",
        "Tout le processus peut être représenté par la formule suivante:\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Linear}\\big(\\text{Concat}(\\text{head}_1, ..., \\text{head}_h)\\big)$$\n",
        "\n",
        "où\n",
        "$$\\text{head}_i= \\text{softmax}\\left(\\frac{Q^{(i)} {K^{(i)}}^T} {\\sqrt{d_i}}\\right) \\cdot V^{(i)}$$\n",
        "\n",
        "Illustration:\n",
        "![wholepic](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/mha-illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gHT8q584YqGU"
      },
      "source": [
        "### Exercice: Implémentation de la couche single-head self-attention\n",
        "Pour faire produit matriciel entre deux matrices `a` et `b`, vous pouvez utiliser `torch.matmul(a, b)`.\n",
        "\n",
        "Essayez de compléter la fonction `forward` de la cellule ci-dessous pour implémenter la couche single-head self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wsfwz4rYqGU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class SingleHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "        q = self.q_linear(x) # q.shape = (batch_size, seq_len, d_model)\n",
        "        k = self.k_linear(x) # k.shape = (batch_size, seq_len, d_model)\n",
        "        v = self.v_linear(x) # v.shape = (batch_size, seq_len, d_model)\n",
        "\n",
        "        k_transpose = k.transpose(-2, -1) # k est de shape (batch_size, seq_len, d_model), et on veut permuter les deux dernières dimensions d'où -2 et -1\n",
        "        # k_transpose.shape = (batch_size, d_model, seq_len)\n",
        "\n",
        "        scores = self.softmax(torch.matmul(q, k_transpose) / math.sqrt(self.d_model))\n",
        "\n",
        "        return torch.matmul(scores, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cD9QdH8ZYqGU"
      },
      "source": [
        "### Vérification\n",
        "\n",
        "Lorsque vous lancez la cellule suivante, vous devriez obtenir\n",
        "```python\n",
        "tensor([[[-2.5449, -0.0960],\n",
        "          [-2.6525, -0.1210]]])\n",
        "```\n",
        "comme résultat. (ignorez le `grad_fn`)\n",
        "Si vous obtenez un résultat différent, vérifiez bien votre implémentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58uON7yMYqGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d624c276-9f3f-4cca-c794-ca4aba11387f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-2.5449, -0.0960],\n",
            "         [-2.6525, -0.1210]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "sa = SingleHeadSelfAttention(2)\n",
        "x = torch.tensor([[[1, 2], [3, 4]]], dtype=torch.float)\n",
        "\n",
        "print(sa(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mHj4ITYRYqGU"
      },
      "source": [
        "### Exercice: Implémentation de la couche multi-head self-attention\n",
        "Pour split un tenseur en plusieurs parties, regardez la documentation de `torch.split`: https://pytorch.org/docs/stable/generated/torch.split.html\n",
        "\n",
        "Pour concaténer plusieurs tenseurs, regardez la documentation de `torch.cat`: https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "\n",
        "Vérifiez bien sur quelle dimension vous faites l'opération.\n",
        "Il faudra utiliser une boucle `for` pour appliquer la self-attention à chaque head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H2B8qZvYqGU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, h):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.heads = [SingleHeadSelfAttention(d_model//h) for _ in range(h)]\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # lorsque vous avez une liste de couche, il faut transformer cette liste en un module nn.ModuleList, sinon les paramètres de ces couches ne seront pas enregistrés par PyTorch\n",
        "        # si vous ne faites pas ça, lorsque vous faites model.to(\"cuda\"), les paramètres de ces couches ne seront pas envoyés sur le GPU automatiquement\n",
        "        self.heads = nn.ModuleList(self.heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "\n",
        "        x_dec = torch.split(x, self.h, dim = -1)\n",
        "\n",
        "        heads = []\n",
        "        for i in range(self.h):\n",
        "            heads.append(self.heads[i](x_dec[i]))\n",
        "\n",
        "        v_concat = torch.cat(heads, dim = -1)\n",
        "        s = self.linear(v_concat)\n",
        "\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Gf4H5YjlYqGV"
      },
      "source": [
        "### Vérification\n",
        "Pour vérifier que votre implémentation est correcte, vous pouvez lancer la cellule suivante. Vous devriez obtenir\n",
        "\n",
        "```python\n",
        "tensor([[[ 0.3529,  2.5714,  0.9558, -1.3315],\n",
        "         [ 0.4814,  2.6746,  1.2625, -1.2092]]])\n",
        "```\n",
        "comme résultat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7tyINaCYqGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9867d81c-529e-40ba-f503-a771ef463a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.3529,  2.5714,  0.9558, -1.3315],\n",
            "         [ 0.4814,  2.6746,  1.2625, -1.2092]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "mha = MultiHeadSelfAttention(4, 2)\n",
        "x = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8]]], dtype=torch.float)\n",
        "\n",
        "print(mha(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DXgmVm6TYqGV"
      },
      "source": [
        "### Progrès sur l'implémentation du Transformer\n",
        "Maintenant, notre classe `Transformer` ressemble à ça:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz5gZItrYqGV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        outputs = self.mha(inputs) # (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Pkw9d4yXYqGV"
      },
      "source": [
        "### Feedforward\n",
        "![feedforward](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/feedforward.png)\n",
        "\n",
        "La couche feedforward contient simplement deux couches linéaires avec une activation GELU entre les deux. C'est relativement simple comparé au mécanisme d'attention.\n",
        "\n",
        "Chaque vecteur de la séquence de la sortie de la couche MHA va être transformé par la première couche linéaire, qui projette l'entrée dans un espace de dimension plus élevée, typiquement à une dimension 4 fois plus grande que la dimension originale c'est-à-dire `4*d_model`. C'est pour créer une représentation \"plus riche\" de chaque mot.\n",
        "\n",
        "Ensuite, on utilise une deuxième couche linéaire pour transformer les vecteurs de grande dimension vers la dimension originale `d_model`.\n",
        "\n",
        "![ff](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/ff.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0In8JGREYqGV"
      },
      "source": [
        "### Exercice: Implémentation de la couche feedforward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "DgL6j-UwYqGW"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.linear1 = nn.Linear(d_model, 4*d_model)\n",
        "        self.linear2 = nn.Linear(4*d_model, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dPphRSuOYqGW"
      },
      "source": [
        "### Exercice: Ajouter la couche feedforward au Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2dndNCnYqGW"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.d_model = d_model\n",
        "        self.fforward = FeedForward(d_model)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        outputs = self.mha(inputs) # (batch_size, seq_len, d_model)\n",
        "        outputs = self.fforward(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Yl0bM67YYqGW"
      },
      "source": [
        "### Add & Norm\n",
        "![addnorm](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/addnorm.png)\n",
        "\n",
        "Cette couche aide à stabiliser les sorties des couches Multi-Head Attention et Feedforward et facilite aussi l'entraînement de notre modèle.\n",
        "\n",
        "Elle consiste en deux opérations principales:\n",
        "\n",
        "- Residual Connection (Add): Les sorties de la couche Multi-Head Attention et de la couche Feedforward sont chacune ajoutées à leur entrée originale (avant ces couches), créant ainsi une connexion résiduelle, qui aident à éviter les problèmes de disparition du gradient.\n",
        "\n",
        "- Layer Normalization (Norm): La connexion résiduelle est suivie d'une couche de normalisation (quoi que, maintenant c'est plus commun de faire la normalisation avant de faire la connection résiduelle). Contrairement à la Batch Normalization, qui normalise sur la dimension du batch, la Layer Normalization s'effectue sur la dimension des features (la dimension `d_model`). Cette normalisation stabilise l'apprentissage du réseau et conduit à une convergence plus rapide et à une meilleure généralisation. La Layer Normalization contient aussi deux paramètres pour ajuster la moyenne et l'écart-type, qui seront appris durant l'entraînement, tout comme la Batch Normalization. La documentation de PyTorch pour la Layer Normalization est [ici](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "\n",
        "Gif qui illsutre:\n",
        "![addnorm gif](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/addnorm.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eRlaaUe2YqGW"
      },
      "source": [
        "### Implémentation de la connexion résiduelle et de la LayerNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkSoOEK5YqGW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.feedforward = FeedForward(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        mha_outputs = self.mha(inputs) # (batch_size, seq_len, d_model)\n",
        "        mha_outputs = self.layer_norm(mha_outputs + inputs) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        encoder_outputs = self.feedforward(mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        encoder_outputs = self.layer_norm(encoder_outputs + mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        return encoder_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5DTsAms3YqGW"
      },
      "source": [
        "### Plusieurs Encoder Layers\n",
        "![encoder](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/encoder.png)\n",
        "\n",
        "En fait, tout ce qu'on a fait jusqu'à maintenant est juste la partie Encodeur du Transformer. Le Transformer est une sorte de Encodeur-Décodeur.\n",
        "Mais le transformer ne contient pas qu'une seule couche Encodeur, il contient plusieurs couches Encodeur.\n",
        "\n",
        "![stack](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/stack.png)\n",
        "\n",
        "Chaque couche Encodeur contient une couche Multi-Head Attention, une couche Feedforward, et une couche Add & Norm. Chaque couche prend en entrée la sortie de la couche précédente.\n",
        "Cette architecture permet au modèle d'extraire et de traiter des représentations de plus en plus abstraites des données d'entrée, apprenant ainsi des motifs et des relations complexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KBxQdd9CYqGX"
      },
      "source": [
        "### Implémentation de plusieurs couches Encodeur\n",
        "Tout d'abord, on va implémenter la couche `EncoderLayer`, et ensuite la couche `TransformerEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMNAN9OwYqGX"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.feedforward = FeedForward(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        mha_outputs = self.mha(inputs) # (batch_size, seq_len, d_model)\n",
        "        mha_outputs = self.layer_norm(mha_outputs + inputs) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        encoder_outputs = self.feedforward(mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        encoder_outputs = self.layer_norm(encoder_outputs + mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        return encoder_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gH5UIZGXYqGX"
      },
      "source": [
        "### Exercice: Implémentation de TransformerEncoder\n",
        "Essayez d'implémenter la couche `TransformerEncoder` en utilisant la couche `EncoderLayer` qu'on vient de créer. `TransformerEncoder` prend en entrée un tenseur de taille `(batch_size, seq_len, d_model)` qui correspond à la sortie de la couche Embedding et de la couche PositionalEncoding. Elle doit retourner un tenseur de taille `(batch_size, seq_len, d_model)` qui correspond à la sortie de la dernière couche Encodeur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8nNnh94YqGX"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers_list = [EncoderLayer(d_model, num_heads) for _ in range(num_layers)]\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.layers_list = nn.ModuleList(self.layers_list)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layer_list:\n",
        "          inputs = layer(inputs)\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mmgKO8lnYqGX"
      },
      "source": [
        "### Exercice: Modification de la classe `Transformer`\n",
        "Maintenant, on va modifier la classe `Transformer` pour qu'elle utilise la classe `TransformerEncoder` qu'on vient de créer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcBWX7EgYqGX"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, input_ids, decoder_inputs_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "        encoded = self.encoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2ALtb_SAYqGY"
      },
      "source": [
        "### Décodeur\n",
        "![decoder](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/decoder.png)\n",
        "\n",
        "### Exercice: Implémentation de OutputEmbedding et PositionalEncoding\n",
        "Avant de commencer à regarder le décodeur, il faut implémenter la couche `OutputEmbedding` et la couche `PositionalEncoding` pour le décodeur.\n",
        "![outputembed](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/outputembed.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxiQ2WuEYqGY"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab1_size, vocab2_size, d_model, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_embedding = nn.Embedding(vocab1_size, d_model)\n",
        "        self.output_embedding = nn.Embedding(vocab2_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.encoder = TransformerEncoder(d_model, num_heads, num_layers)\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, input_ids, decoder_inputs_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.input_embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "        encoded = self.encoder()\n",
        "\n",
        "        outputs = self.output_embedding(decoder_inputs_ids)\n",
        "        outputs = self.pos_enc(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mb-oDfTgYqGY"
      },
      "source": [
        "### Multi-Head Self Attention avec Mask\n",
        "![maskmha](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/maskmha.png)\n",
        "\n",
        "On a vu que le mécanisme Multi-Head Self Attention joue un rôle essentiel pour mettre en relation les vecteurs dans une séquence. Cependant, lors de la prédiction d'un token pendant la phase d'entraînement, comme on lui donne en entrée toute la phrase cible (target sequence), on ne veut pas que le modèle puisse tricher: on veut s'assurer que la prédiction pour un token particulier ne dépend pas des mots qui viennent après.\n",
        "Exemple:\n",
        "- Supponsons qu'on veut prédire le mot \"manger\" dans la phrase \"J'aime manger des oranges.\". Comme on a dit précédemment, on veut que le transformer n'utilise que [J', aime] pour prédire \"manger\". S'il avait accès à toute la phrase \"J'aime manger des oranges.\", il pourrait juste regarder le mot qui suit \"aime\" pour prédire.\n",
        "\n",
        "C'est là qu'intervient le Masked Multi-Head Self Attention.\n",
        "\n",
        "Le Masked Multi-Head Self Attention est très similaire à Multi-Head Self Attention standard avec juste petite une différence: on additionne un mask sur la matrice des scores d'attention (avant le softmax) pour que les positions futures ne soient pas prises en compte lors du calcul de l'attention.\n",
        "\n",
        "Illustration:\n",
        "\n",
        "![causal_mask](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/causal_mask.png)\n",
        "\n",
        "\n",
        "![causal_mask_after](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/causal_mask_after.png)\n",
        "\n",
        "\n",
        "On construit ce mask en définissant des valeurs négatives extrêmement grandes (souvent -inf ou -1e9) aux positions pertinentes dans la matrice des scores d'attention, juste avant l'étape softmax, et en mettant des 0 aux autres positions. Ça fait que la fonction softmax produit des 0 pour ces positions, ignorant ainsi efficacement les positions masquées lors du calcul de l'attention. On remarque que si on veut pour que modèle ne puisse pas tricher, on va devoir utiliser un mask qui est une matrice triangulaire avec des `-inf` sur la partie supérieure de la matrice et des 0 sur la partie inférieure. On appelle ce type de mask un **causal mask**:\n",
        "\n",
        "![matrixcausal](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/matrixcausal.png)\n",
        "\n",
        "Je vous laisse faire le calcul pour vérifier que le mask empêche bien le modèle de tricher.\n",
        "\n",
        "![diff](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/diff.png)\n",
        "\n",
        "![example](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/example.png)\n",
        "\n",
        "Pour l'exemple ci-dessus, si vous faites les calculs, vous verrez que lors du self-attention du mot \"am\", le mot \"fine\" n'est pas pris en compte.\n",
        "\n",
        "La formule finale pour le masked single-head self attention est la suivante: $$\\text{new V} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}} + \\text{mask}\\right)V$$\n",
        "\n",
        "Remarque: Bien qu'on a ajouté le mask pour l'entraînement, on ne peut pas l'enlever pendant l'inférence. C'est parce que le modèle est entraîné à utiliser seulement les tokens précédents pour prédire le token suivant. Si on enlève le mask, le modèle va utiliser les tokens futurs pour faire l'attention sur les tokens précédents, ce qui peut donner des résultats incohérents car il n'est jamais entraîné à faire ça.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WtohZLa4YqGY"
      },
      "source": [
        "### Exercice: Implémentation de la classe `MaskedSingleHeadAttention`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ota3ElDPYqGY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "def get_causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    seq_len: int\n",
        "    \"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    mask = mask.masked_fill(mask==1, float('-inf'))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class MaskedSingleHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        causal_mask = get_causal_mask(seq_len) # (seq_len, seq_len)\n",
        "\n",
        "        q = self.q_linear(x) # q.shape = (batch_size, seq_len, d_model)\n",
        "        k = self.k_linear(x) # k.shape = (batch_size, seq_len, d_model)\n",
        "        v = self.v_linear(x) # v.shape = (batch_size, seq_len, d_model)\n",
        "\n",
        "        k_transpose = k.transpose(-2, -1) # k est de shape (batch_size, seq_len, d_model), et on veut permuter les deux dernières dimensions d'où -2 et -1\n",
        "        # k_transpose.shape = (batch_size, d_model, seq_len)\n",
        "\n",
        "        scores = self.softmax(torch.matmul(q, k_transpose) / math.sqrt(self.d_model) + causal_mask)\n",
        "\n",
        "        return torch.matmul(scores, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8_z3FrckYqGY"
      },
      "source": [
        "### Vérification\n",
        "Si vous avez bien implémenté la classe `MaskedSingleHeadAttention`, vous devriez obtenir\n",
        "```python\n",
        "\n",
        "tensor([[[-1.3326,  0.1852],\n",
        "         [-2.6525, -0.1210],\n",
        "         [-4.3539, -0.5156]]])\n",
        "```\n",
        "\n",
        "en exécutant la cellule ci-dessous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDV5HivVYqGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f9913d-c92c-478d-c022-b026bc9dcc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.3326,  0.1852],\n",
            "         [-2.6525, -0.1210],\n",
            "         [-4.3539, -0.5156]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "sa = MaskedSingleHeadAttention(2)\n",
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]]], dtype=torch.float)\n",
        "\n",
        "print(sa(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ginY1XBlYqGZ"
      },
      "source": [
        "### Exercice: Implémentation de la classe `MaskedMultiHeadAttention`\n",
        "La couche `MaskedMultiHeadAttention` ressemble énormément à la couche `MultiHeadAttention` qu'on a implémenté précédemment. La seule différence est qu'on va utiliser la classe `MaskedSingleHeadAttention` qu'on vient d'implémenter au lieu de `SingleHeadAttention`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezrlE7RfYqGZ"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, h):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.heads = [MaskedSingleHeadAttention(d_model//h) for _ in range(h)]\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # lorsque vous avez une liste de couche, il faut transformer cette liste en un module nn.ModuleList, sinon les paramètres de ces couches ne seront pas enregistrés par PyTorch\n",
        "        # si vous ne faites pas ça, lorsque vous faites model.to(\"cuda\"), les paramètres de ces couches ne seront pas envoyés sur le GPU automatiquement\n",
        "        self.heads = nn.ModuleList(self.heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "\n",
        "        x_dec = torch.split(x, self.h, dim = -1)\n",
        "\n",
        "        heads = []\n",
        "        for i in range(self.h):\n",
        "            heads.append(self.heads[i](x_dec[i]))\n",
        "\n",
        "        v_concat = torch.cat(heads, dim = -1)\n",
        "        s = self.linear(v_concat)\n",
        "\n",
        "        return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IJS6EvSPYqGZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IWDC9BtkYqGZ"
      },
      "source": [
        "### Multi-head Cross-Attention\n",
        "![crossatt](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/crossatt.png)\n",
        "\n",
        "Le Multi-Head Cross Attention est un mécanisme utilisé pour faire l'attention entre la sortie de l'encodeur et l'entrée du décodeur (plus précisément la sortie de la couche Multi head Self Attention du décodeur).\n",
        "\n",
        "Dans le Multi-Head Self Attention standard, les queries, keys et values viennent tous du même vecteur. Cependant, dans le Multi-Head Cross Attention, les queries viennent de la couche précédente du décodeur, et les keys et values viennent de la sortie de l'encodeur. Ça permet à chaque token de le décodeur de faire attention à toutes les tokens de la séquence source, d'où \"cross-attention\".\n",
        "\n",
        "![illuscross](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/illuscross.png)\n",
        "\n",
        "Pour de la traduction, on peut imaginer que le décodeur cherche des key words de la séquence source pour savoir quel mot traduire ensuite.\n",
        "Par exemple, si la séquence source est \"I love eating oranges.\", lorsqu'on essaie de prédire le mot qui suit `J'`, le modèle va chercher des key words dans la séquence source, et va trouver `I` et `love` qui ont une forte corrélation avec `J'`, et il saura donc que le mot suivant va être le verbe `aimer` et qu'il faut conjuguer à la première personne du singulier.\n",
        "\n",
        "La formule pour cross-attention est la même que celle pour self-attention, sauf que les queries viennent du décodeur et les keys et values viennent de l'encodeur. On a donc:\n",
        "$$\\text{score}=\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)$$\n",
        "$$V'=\\text{score} \\cdot V$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1qpDqd4sYqGZ"
      },
      "source": [
        "### Exercice: Implémentation de la classe `SingleHeadCrossAttention`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRBggqLQYqGa"
      },
      "outputs": [],
      "source": [
        "class SingleHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "\n",
        "    def forward(self, decoder_output, encoder_output):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        q = self.q_linear(decoder_output) # q.shape = (batch_size, seq_len_x, d_model)\n",
        "        k = self.k_linear(encoder_output) # k.shape = (batch_size, seq_len_y, d_model)\n",
        "        v = self.v_linear(encoder_output) #v.shape = (batch_size, seq_len_y, d_model)\n",
        "        score = self.softmax(torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_model))\n",
        "        output = torch.matmul(score, v)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5XVV_zppYqGa"
      },
      "source": [
        "### Vérification\n",
        "Normalement vous devriez obtenir le résultat suivant:\n",
        "```python\n",
        "tensor([[[-4.0098, -0.7054],\n",
        "         [-4.2934, -0.7711],\n",
        "         [-4.5612, -0.8332]]])\n",
        "```\n",
        "en exécutant la cellule ci-dessous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhbF9nR4YqGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df524b9e-37e2-45d1-c302-85a46967c670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-4.0098, -0.7054],\n",
            "         [-4.2934, -0.7711],\n",
            "         [-4.5612, -0.8332]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "ca = SingleHeadCrossAttention(2)\n",
        "decoder_output = torch.tensor([[[1, 2], [3, 4], [5, 6]]], dtype=torch.float)\n",
        "encoder_output = torch.tensor([[[6, 5], [4, 3], [2, 1]]], dtype=torch.float)\n",
        "\n",
        "print(ca(decoder_output, encoder_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8JODSO8JYqGa"
      },
      "source": [
        "### Implémentation de la classe `MultiHeadCrossAttention`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvJbGKAIYqGa"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.attention_heads = nn.ModuleList([SingleHeadCrossAttention(d_model//n_heads) for _ in range(n_heads)])\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        heads = [head(decoder_input, encoder_output) for head in self.attention_heads]\n",
        "        heads = torch.cat(heads, dim=-1)\n",
        "        output = self.linear(heads)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Yvl64j6XYqGa"
      },
      "source": [
        "### Overview du décodeur\n",
        "![decoderr](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/decoderr.png)\n",
        "\n",
        "On empile aussi les décodeurs comme on a empilé les encodeurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QRvhC3lpYqGb"
      },
      "source": [
        "### Exercice: Implémentation de la classe `DecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P2dX_3cYqGb"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.mmha = MaskedMultiHeadAttention(d_model, n_heads)\n",
        "        self.mhca = MultiHeadCrossAttention(d_model, n_heads)\n",
        "        self.feedforward = FeedForward(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, decoder_output, encoder_output):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        mmha_outputs = self.mmha(decoder_output) # (batch_size, seq_len_x, d_model)\n",
        "        mmha_outputs = self.layer_norm(mmha_outputs + decoder_output) # (batch_size, seq_len_x, d_model)\n",
        "\n",
        "        mhca_outputs = self.mhca(mmha_outputs + encoder_output)\n",
        "        mhca_outputs = self.layer_norm(mhca_outputs + mmha_outputs)\n",
        "\n",
        "        decoder_outputs = self.feedforward(mhca_outputs)\n",
        "        decoder_outputs = self.layer_norm(decoder_outputs + mhca_outputs)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "L-ivNfS5YqGb"
      },
      "source": [
        "### Exercice: Implémentation de la classe `TransformerDecoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g_c6RdTYqGb"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers_list = [DecoderLayer(d_model, n_heads) for _ in range(n_layers)]\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.layers_list = nn.ModuleList(self.layers_list)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers_list:\n",
        "          decoder_input = layer(decoder_input, encoder_output)\n",
        "        return decoder_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O4pnzIAcYqGb"
      },
      "source": [
        "### Exercice: Mettre à jour la classe `Transformer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SxvNHDRYqGb"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab1_size, vocab2_size, d_model, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_embedding = nn.Embedding(vocab1_size, d_model)\n",
        "        self.output_embedding = nn.Embedding(vocab2_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.encoder = TransformerEncoder(d_model, num_heads, num_layers)\n",
        "        self.decoder = TransformerDecoder(d_model, num_heads, num_layers)\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, input_ids, decoder_inputs_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        decoder_input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        inputs = self.input_embedding(input_ids) # (batch_size, seq_len, d_model)\n",
        "        inputs = self.pos_enc(inputs) # (batch_size, seq_len, d_model)\n",
        "        encoded = self.encoder()\n",
        "        outputs = self.output_embedding(decoder_inputs_ids)\n",
        "        outputs = self.pos_enc(outputs)\n",
        "        decoded = self.decoder()\n",
        "\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bUtT0DdUYqGb"
      },
      "source": [
        "### Presque fini: La dernière couche et le softmax\n",
        "![lastlayer](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/lastlayer.png)\n",
        "\n",
        "La dernière couche est juste une couche linéaire + softmax pour prédire les mots suivants. Elle prend en entrée la sortie du décodeur qui est de taille `(batch_size, seq_len, d_model)` et renvoie un tenseur de taille `(batch_size, seq_len, vocab_size)` qui correspond à la probabilité d'avoir chaque mot du vocabulaire à chaque position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "U-2sVTX2YqGb"
      },
      "source": [
        "### Implémentation complète de la classe `Transformer`\n",
        "\n",
        "Pas besoin de softmax dans notre implémentation car il est déjà inclus dans la loss `CrossEntropyLoss` de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meM2mWM6YqGc"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab1_size, vocab2_size, d_model, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.input_embedding = nn.Embedding(vocab1_size, d_model)\n",
        "        self.output_embedding = nn.Embedding(vocab2_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        self.encoder = TransformerEncoder(d_model, n_heads, n_layers)\n",
        "        self.decoder = TransformerDecoder(d_model, n_heads, n_layers)\n",
        "        self.last_layer = nn.Linear(d_model, vocab2_size)\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len_x)\n",
        "        decoder_input_ids: (batch_size, seq_len_y)\n",
        "        \"\"\"\n",
        "        encoder_input = self.input_embedding(input_ids)\n",
        "        decoder_input = self.output_embedding(decoder_input_ids)\n",
        "\n",
        "        encoder_input = self.pos_enc(encoder_input)\n",
        "        decoder_input = self.pos_enc(decoder_input)\n",
        "\n",
        "        encoder_output = self.encoder(encoder_input)\n",
        "        decoder_output = self.decoder(decoder_input, encoder_output) # (batch_size, seq_len_y, d_model)\n",
        "        output = self.last_layer(decoder_output) # (batch_size, seq_len_y, vocab_size)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tpnb0WP8YqGc"
      },
      "source": [
        "### Mdr non je vous ai menti, c'est pas fini\n",
        "### Key padding mask\n",
        "En NLP, un problème courant est de travailler avec des phrases de longueurs variables. Certaines phrases peuvent être relativement courtes, mais d'autres peuvent être beaucoup plus longues. Cependant, notre modèle s'attend à ce que toutes les entrées du batch soient de la même longueur.\n",
        "\n",
        "On va devoir alors utiliser le padding : on ajouter des tokens `<pad>` à toutes les phrases du batch pour qu'elles aient toutes la même longueur.\n",
        "\n",
        "Ces tokens `<pad>` sont artificiels et ne portent aucune signification sémantique réelle, et donc on ne veut pas que notre modèle prête attention à ces tokens dans la couche Multi head Attention. C'est là que les \"key padding mask\" vont nous aider.\n",
        "\n",
        "Un key padding mask est un tenseur binaire de taille `(batch_size, max_seq_len)` qui indique où se trouvent les tokens `<pad>` dans chaque phrase.\n",
        "Dans ce tenseur, la valeur 1 indique que le token est un `<pad>` et la valeur 0 indique que le token n'est pas un `<pad>`.\n",
        "\n",
        "Par exemple, considérons une phrase \"J'aime manger des oranges\", qui a été pad à la longueur 7 : \"J'aime manger des oranges \\<pad> \\<pad> \\<pad>\". Le key padding mask correspondant serait : `[0 0 0 0 1 1 1]`.\n",
        "\n",
        "Lors du calcul des scores d'attention, on va utiliser ce mask pour s'assurer que le modèle ne prête pas attention aux tokens de padding. On fait ça en mettant les scores d'attention des paddings à `-inf` avant d'appliquer la fonction softmax.\n",
        "\n",
        "Le softmax de ces scores sera 0, et donc les poids d'attention pour paddings seront également nuls.\n",
        "\n",
        "Voici une illustration de la façon dont key padding mask fonctionne dans la couche Multi head attention:\n",
        "\n",
        "![softmax](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/softmax.png)\n",
        "\n",
        "Dans l'exemple ci-dessus, on suppose que $K_3$ et $K_4$ sont des tokens de padding. On s'en fiche si $Q_i$ est un token de padding ou non. On ne veut juste pas que $K_3$ et $K_4$ influencent les poids d'attention. On va donc mettre les scores d'attention de $K_3$ et $K_4$ à `-inf` avant d'appliquer le softmax.\n",
        "\n",
        "Si vous faites le calcul, vous verrez que quand vous calculez $\\text{new }V_1$, $\\text{new } V_2$ et $\\text{new } V_3$ et $\\text{new } V_4$, les poids d'attention de $K_3$ et $K_4$ n'infuencent pas la somme pondérée.\n",
        "\n",
        "Pourquoi on s'en fiche que $Q_i$ soit un padding ou non?\n",
        "Car les poids d'attention avec $Q_i$ ne sont utilisés que pour calculer $\\text{new } V_i$, et pas d'autres $\\text{new } V_j$. Vous pouvez faire le calcul pour vérifier.\n",
        "\n",
        "Dans le self-attention et le cross-attention, si $Q_i$ est un token de padding, alors $V_i$ l'est aussi. Donc on s'en fiche si les poids d'attention avec $Q_i$ influencent $V_i$, car $V_i$ ne porte pas de signification sémantique réelle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9rn8MoKSYqGc"
      },
      "source": [
        "### Implémentation du key padding mask sur `SingleHeadSelfAttention`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us2ZxmboYqGc"
      },
      "outputs": [],
      "source": [
        "class SingleHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Q: (batch_size, seq_len_q, d_model)\n",
        "        K: (batch_size, seq_len_k, d_model)\n",
        "        V: (batch_size, seq_len_v, d_model)\n",
        "        key_padding_mask: (batch_size, seq_len_k)\n",
        "        \"\"\"\n",
        "        Q = self.q_linear(x) # (batch_size, seq_len_q, d_model)\n",
        "        K = self.k_linear(x) # (batch_size, seq_len_k, d_model)\n",
        "        V = self.v_linear(x) # (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(1, 2)) # (batch_size, seq_len_q, seq_len_k)\n",
        "        scores = scores / math.sqrt(self.d_model)\n",
        "\n",
        "        # On met les scores d'attention des paddings à -inf\n",
        "        if key_padding_mask is not None:\n",
        "            scores = scores.masked_fill(key_padding_mask.unsqueeze(1) == 1, -1e9)\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1) # (batch_size, seq_len_q, seq_len_k)\n",
        "        output = torch.matmul(weights, V) # (batch_size, seq_len_q, d_model)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2ErTuViFYqGc"
      },
      "source": [
        "### Exercice: Réimplémentez `MultiHeadSelfAttention`, `SingleHeadCrossAttention`, `MultiHeadCrossAttention` et `Transformer` avec le key padding mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ah_qzydYqGc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, h):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.heads = [SingleHeadSelfAttention(d_model//h) for _ in range(h)]\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # lorsque vous avez une liste de couche, il faut transformer cette liste en un module nn.ModuleList, sinon les paramètres de ces couches ne seront pas enregistrés par PyTorch\n",
        "        # si vous ne faites pas ça, lorsque vous faites model.to(\"cuda\"), les paramètres de ces couches ne seront pas envoyés sur le GPU automatiquement\n",
        "        self.heads = nn.ModuleList(self.heads)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        # x.shape = (batch_size, seq_len, d_model)\n",
        "        l_x = torch.split(x, split_size_or_sections=self.d_model//self.h, dim=-1)\n",
        "        x = torch.cat([self.heads[i](l_x[i], key_padding_mask) for i in range(self.h)], dim = 2)\n",
        "\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SingleHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, decoder_output, encoder_output, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        # decoder_output.shape = (batch_size, seq_len, d_model)\n",
        "        q = self.q_linear(decoder_output) # q.shape = (batch_size, seq_len, d_model)\n",
        "        k = self.k_linear(encoder_output)\n",
        "        v = self.v_linear(encoder_output)\n",
        "        k_transpose = k.transpose(-2, -1) # k est de shape (batch_size, seq_len, d_model), et on veut permuter les deux dernières dimensions d'où -2 et -1\n",
        "        # k_transpose.shape = (batch_size, d_model, seq_len)\n",
        "        scores = torch.matmul(q, k_transpose)/(self.d_model**(1/2))\n",
        "        # On met les scores d'attention des paddings à -inf\n",
        "        if key_padding_mask is not None:\n",
        "            scores = scores.masked_fill(key_padding_mask.unsqueeze(1) == 1, -1e9)\n",
        "\n",
        "        scores = torch.matmul(self.softmax(scores),v)\n",
        "\n",
        "        return scores\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.attention_heads = nn.ModuleList([SingleHeadCrossAttention(d_model//n_heads) for _ in range(n_heads)])\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        l_decoder = torch.split(decoder_input, split_size_or_sections=self.d_model//self.n_heads, dim=-1)\n",
        "        l_encoder = torch.split(encoder_output, split_size_or_sections=self.d_model//self.n_heads, dim=-1)\n",
        "        heads = [self.attention_heads[i](l_decoder[i], l_encoder[i], key_padding_mask) for i in range(self.n_heads)]\n",
        "        heads = torch.cat(heads, dim=-1)\n",
        "        output = self.linear(heads)\n",
        "        return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.feedforward = FeedForward(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def forward(self, inputs, src_key_padding_mask = None):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        mha_outputs = self.mha(inputs, src_key_padding_mask) # (batch_size, seq_len, d_model)\n",
        "        mha_outputs = self.layer_norm(mha_outputs + inputs) # (batch_size, seq_len, d_model)\n",
        "        encoder_outputs = self.feedforward(mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        encoder_outputs = self.layer_norm(encoder_outputs + mha_outputs) # (batch_size, seq_len, d_model)\n",
        "        return encoder_outputs\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers_list = [EncoderLayer(d_model, num_heads) for _ in range(num_layers)]\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.layers_list = nn.ModuleList(self.layers_list)\n",
        "\n",
        "    def forward(self, inputs, src_key_padding_mask = None):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers_list:\n",
        "            inputs = layer(inputs, src_key_padding_mask)\n",
        "        return inputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.mmha = MaskedMultiHeadAttention(d_model, n_heads)\n",
        "        self.mhca = MultiHeadCrossAttention(d_model, n_heads)\n",
        "        self.feedforward = FeedForward(d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, decoder_output, encoder_output, src_key_padding_mask = None):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        mmha_outputs = self.mmha(decoder_output) # (batch_size, seq_len, d_model)\n",
        "        mmha_outputs = self.layer_norm(mmha_outputs + decoder_output) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        mhca_outputs = self.mhca(decoder_output, encoder_output, src_key_padding_mask)\n",
        "        mhca_outputs = self.layer_norm(mhca_outputs + mmha_outputs)\n",
        "\n",
        "        outputs = self.feedforward(mhca_outputs) # (batch_size, seq_len, d_model)\n",
        "        outputs = self.layer_norm(outputs + mhca_outputs) # (batch_size, seq_len, d_model)\n",
        "        return outputs\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers_list = [DecoderLayer(d_model, n_heads) for _ in range(n_layers)]\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.layers_list = nn.ModuleList(self.layers_list)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output, src_key_padding_mask = None):\n",
        "        \"\"\"\n",
        "        decoder_output: (batch_size, seq_len_x, d_model)\n",
        "        encoder_output: (batch_size, seq_len_y, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers_list:\n",
        "            decoder_input = layer(decoder_input, encoder_output, src_key_padding_mask)\n",
        "        return decoder_input\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab1_size, vocab2_size, d_model, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.input_embedding = nn.Embedding(vocab1_size, d_model)\n",
        "        self.output_embedding = nn.Embedding(vocab2_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        self.encoder = TransformerEncoder(d_model, n_heads, n_layers)\n",
        "        self.decoder = TransformerDecoder(d_model, n_heads, n_layers)\n",
        "        self.last_layer = nn.Linear(d_model, vocab2_size)\n",
        "\n",
        "    def forward(self, input_ids, decoder_input_ids, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len_x)\n",
        "        decoder_input_ids: (batch_size, seq_len_y)\n",
        "        \"\"\"\n",
        "        encoder_input = self.input_embedding(input_ids)\n",
        "        decoder_input = self.output_embedding(decoder_input_ids)\n",
        "\n",
        "        encoder_input = self.pos_enc(encoder_input)\n",
        "        decoder_input = self.pos_enc(decoder_input)\n",
        "\n",
        "        encoder_output = self.encoder(encoder_input, src_key_padding_mask)\n",
        "        decoder_output = self.decoder(decoder_input, encoder_output, src_key_padding_mask) # (batch_size, seq_len_y, d_model)\n",
        "        output = self.last_layer(decoder_output) # (batch_size, seq_len_y, vocab_size)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding mask dans la loss\n",
        "Considérons la phrase traduite avec pad: [`<start>`, `J'aime`, `manger`, `des`, `oranges`, `<end>`, `<pad>`, `<pad>`, `<pad>`]\n",
        "\n",
        "Alors on met en entrée du modèle la phrase sans le dernier padding: [`<start>`, `J'aime`, `manger`, `des`, `oranges`, `<end>`, `<pad>`, `<pad>`] et on veut que le modèle prédise la phrase sans le premier padding: [`J'aime`, `manger`, `des`, `oranges`, `<end>`, `<pad>`, `<pad>`].\n",
        "\n",
        "Mais si on fait ça, on va forcer le modèle à prédire le token `<pad>` à la fin de la phrase, alors que c'est inutile comme pendant l'inférence, on arrête la boucle dès qu'on obtient le token `<end>`.\n",
        "\n",
        "Donc on va utiliser un mask de padding pour la loss, qui va masquer les prédictions du modèle pour les tokens de padding.\n",
        "\n",
        "Supposons qu'on veut que le modèle prédise [`J'aime`, `manger`, `des`, `oranges`, `<end>`, `<pad>`, `<pad>`], alors on va faire la loss entre les prédictions du modèle et la target sequence: on obtient 7 losses: [$\\ell_1$, $\\ell_2$, $\\ell_3$, $\\ell_4$, $\\ell_5$, $\\ell_6$, $\\ell_7$].\n",
        "\n",
        "Avant de faire la moyenne de cette liste, on va enlever les $\\ell_6$ et $\\ell_7$ car ce sont les losses pour les tokens de padding.\n",
        "\n",
        "L'idée c'est de multiplier la liste de loss avec [1, 1, 1, 1, 1, 0, 0] pour enlever les losses pour les tokens de padding.\n",
        "\n",
        "On peut obtenir la liste [1, 1, 1, 1, 1, 0, 0] en cherchant les tokens `<pad>` dans la target sequence."
      ],
      "metadata": {
        "collapsed": false,
        "id": "cr_F8IyxYqGc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "obYGt4frYqGd"
      },
      "source": [
        "## Application simple\n",
        "On va essayer d'entraîner un transformer pour la traduction très simple sur un dataset très simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JGnE0HSuYqGd"
      },
      "source": [
        "### Dataset\n",
        "Voici le dataset (mdr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsNlZ0QeYqGd"
      },
      "outputs": [],
      "source": [
        "X_train = [\"I like oranges.\", \"I don't like oranges.\", \"I like apples.\", \"I like bananas.\", \"I don't like pineapples and oranges.\"]\n",
        "Y_train = [\"J'aime les oranges.\", \"Je n'aime pas les oranges.\", \"J'aime les pommes.\", \"J'aime les bananes.\", \"Je n'aime pas les ananas et les oranges.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'instant on test sur un dataset simple pour vérifier que le modèle marche et apprend.\n",
        "\n",
        "On verra dans le prochain tp comment générer des histoires."
      ],
      "metadata": {
        "collapsed": false,
        "id": "CYOeqzCGYqGd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cLKudTvwYqGd"
      },
      "source": [
        "### A vous de jouer\n",
        "Il faudra tokenizer puis padder les phrases, puis créer un transformer et l'entraîner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, '<start>': 1, '<end>': 2, 'I': 3, 'like': 4, 'oranges.': 5, \"don't\": 6, 'apples.': 7, 'bananas.': 8, 'pineapples': 9, 'and': 10, \"J'aime\": 11, 'les': 12, 'Je': 13, \"n'aime\": 14, 'pas': 15, 'pommes.': 16, 'bananes.': 17, 'ananas': 18, 'et': 19}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (20) must match the size of tensor b (13) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-2a8df79abde4>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mY_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-bab7faa93480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, decoder_input_ids, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len_y, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len_y, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-bab7faa93480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-bab7faa93480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mmha_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mmha_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmha_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmha_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-bab7faa93480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, key_padding_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# x.shape = (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ml_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-bab7faa93480>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# x.shape = (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ml_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-3175066879e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, key_padding_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len_v, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len_q, seq_len_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (13) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\"\"\"device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\"\"\"\n",
        "lenTokenized = 13\n",
        "\n",
        "dictTokenize = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2}\n",
        "for i in range(len(X_train)):\n",
        "  for token in X_train[i].split():\n",
        "    if token not in dictTokenize:\n",
        "      dictTokenize[token] = len(dictTokenize)\n",
        "for i in range(len(Y_train)):\n",
        "  for token in Y_train[i].split():\n",
        "    if token not in dictTokenize:\n",
        "      dictTokenize[token] = len(dictTokenize)\n",
        "print(dictTokenize)\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    tsplt = text.split()\n",
        "    return [dictTokenize[token] for token in tsplt] + [dictTokenize[\"<end>\"]] + [dictTokenize[\"<pad>\"]] * (lenTokenized - len(tsplt) - 1)\n",
        "\n",
        "def kpmask(tokens):\n",
        "    return torch.tensor([1 if token != dictTokenize['<pad>'] else 0 for token in tokens])\n",
        "\n",
        "\n",
        "\n",
        "nb_epoch = 1\n",
        "model = Transformer(20, 20, 20, 5, 5)\n",
        "loss_fct = torch.nn.functional.cross_entropy\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "for epoch in range(nb_epoch):\n",
        "  for i in range(len(X_train)):\n",
        "    optimizer.zero_grad()\n",
        "    X = X_train[i]\n",
        "    X_tokenized = torch.tensor(tokenizer(X))\n",
        "    mask = kpmask(X_tokenized)\n",
        "\n",
        "    Y = Y_train[i]\n",
        "    Y_tokenized = torch.tensor(tokenizer(Y))\n",
        "\n",
        "    prediction = model(X_tokenized, Y_tokenized, mask)\n",
        "\n",
        "    loss = loss_fct(prediction, Y_tokenized)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "fE8QQZfjYqGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "190cd256-c116-49b2-e4ac-27f3ff957a85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois que vous avez réussi, vous pouvez essayer de l'entraîner sur un vrai dataset: https://pytorch.org/text/main/datasets.html#iwslt2016"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nhXUJOnLYqGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amélioration de votre score sur le challenge Analyse de sentiment\n",
        "Vous pouvez essayer d'introduire certaines notions vues dans ce tp pour améliorer votre modèle de classification de sentiments."
      ],
      "metadata": {
        "collapsed": false,
        "id": "0UfAywJvYqGd"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2eeb9012c8284105982551ffb1ea36c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13cca85f8a1d407d8205d40fa5d7865d",
              "IPY_MODEL_db43aeb879cf498a9c268cd9acd7cb2e",
              "IPY_MODEL_e63a7d5021f2469e8bc2e7995cbe4610"
            ],
            "layout": "IPY_MODEL_38101c5ae77c4474970b614ed6a7fc48"
          }
        },
        "13cca85f8a1d407d8205d40fa5d7865d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6976d6f37aaf452d99e968b173593313",
            "placeholder": "​",
            "style": "IPY_MODEL_7540e7d3c6b445469cfae14e2a598700",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "db43aeb879cf498a9c268cd9acd7cb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d39b1d1ab5f4f9c9cc85e460f02bf2d",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28bcb4b2e8e64c89a5622d86a5e50404",
            "value": 42
          }
        },
        "e63a7d5021f2469e8bc2e7995cbe4610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9ac13ab3b8e495989138a6da945bbdf",
            "placeholder": "​",
            "style": "IPY_MODEL_80646316a81346db98cfaaefeeb7e152",
            "value": " 42.0/42.0 [00:00&lt;00:00, 1.31kB/s]"
          }
        },
        "38101c5ae77c4474970b614ed6a7fc48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6976d6f37aaf452d99e968b173593313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7540e7d3c6b445469cfae14e2a598700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d39b1d1ab5f4f9c9cc85e460f02bf2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bcb4b2e8e64c89a5622d86a5e50404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9ac13ab3b8e495989138a6da945bbdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80646316a81346db98cfaaefeeb7e152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1daf40c00c6f4066beb777cdba90d87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce423e42d50a4c80beff2f7863c96020",
              "IPY_MODEL_7fa78bfba6ff4c54ad8366da40c31b25",
              "IPY_MODEL_a9061aa455af4b8e880025bf270a4839"
            ],
            "layout": "IPY_MODEL_7e5bd1a5a1e54a8b9aaa1c1fb9b45f3e"
          }
        },
        "ce423e42d50a4c80beff2f7863c96020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8309e4fbdf954bf583bec21662b1ddb7",
            "placeholder": "​",
            "style": "IPY_MODEL_eded5eb555d14f24ba24b896cc088f6f",
            "value": "source.spm: 100%"
          }
        },
        "7fa78bfba6ff4c54ad8366da40c31b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91f60ae75393443db79d3f0ff9f37fe0",
            "max": 778395,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51b79c8e04c742408de64eee64df7e0a",
            "value": 778395
          }
        },
        "a9061aa455af4b8e880025bf270a4839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac12a8d4107416bb4f07fc39303f7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_5eef299efee74f55818c9b921e1f78bf",
            "value": " 778k/778k [00:00&lt;00:00, 5.58MB/s]"
          }
        },
        "7e5bd1a5a1e54a8b9aaa1c1fb9b45f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8309e4fbdf954bf583bec21662b1ddb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eded5eb555d14f24ba24b896cc088f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91f60ae75393443db79d3f0ff9f37fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b79c8e04c742408de64eee64df7e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ac12a8d4107416bb4f07fc39303f7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eef299efee74f55818c9b921e1f78bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a11e93067160472abb3309e8322b36b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d275b399ed034fa6b5a60b7ad65ad9bf",
              "IPY_MODEL_bf3dba6036dd4d21bdc5bccb450b45b3",
              "IPY_MODEL_8fac82ffccbc41529671ee0e806f8027"
            ],
            "layout": "IPY_MODEL_ffd56fde265f49eeba5291f27042d4e5"
          }
        },
        "d275b399ed034fa6b5a60b7ad65ad9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27079775ba7b4a4a9347ec7b9d768312",
            "placeholder": "​",
            "style": "IPY_MODEL_51c1ead0511e45e2a22ea0a2b6ed1f0e",
            "value": "target.spm: 100%"
          }
        },
        "bf3dba6036dd4d21bdc5bccb450b45b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af71551cbae5415fbf719298776d1f0b",
            "max": 802397,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bf7b764ada746b6b1fbe49b5c1bc096",
            "value": 802397
          }
        },
        "8fac82ffccbc41529671ee0e806f8027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4395abed933944f7ba93946050ef1c44",
            "placeholder": "​",
            "style": "IPY_MODEL_7b867a5e78e14909b094fb61d2d608ec",
            "value": " 802k/802k [00:00&lt;00:00, 13.9MB/s]"
          }
        },
        "ffd56fde265f49eeba5291f27042d4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27079775ba7b4a4a9347ec7b9d768312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c1ead0511e45e2a22ea0a2b6ed1f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af71551cbae5415fbf719298776d1f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf7b764ada746b6b1fbe49b5c1bc096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4395abed933944f7ba93946050ef1c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b867a5e78e14909b094fb61d2d608ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e083636fe4dd4bfdb0178ee3934ba88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57086f0faf014b09a15f94a57d359fa3",
              "IPY_MODEL_3bb563b857b74c4bbe06b00253d55589",
              "IPY_MODEL_162dbb76edfa4496810441673067d854"
            ],
            "layout": "IPY_MODEL_f233bf96fb3f4a0c8475b23bbcf4b09f"
          }
        },
        "57086f0faf014b09a15f94a57d359fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4078c832b243cdb885ed1dc5713cfc",
            "placeholder": "​",
            "style": "IPY_MODEL_cbe85e322bc74ee297a01fabc2942757",
            "value": "vocab.json: 100%"
          }
        },
        "3bb563b857b74c4bbe06b00253d55589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3daada782f16478b8b27bbd612d7b940",
            "max": 1339166,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_696699949c9b4e32ae92dbd45db0172b",
            "value": 1339166
          }
        },
        "162dbb76edfa4496810441673067d854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57489af57ceb4874b34561e3b8d06122",
            "placeholder": "​",
            "style": "IPY_MODEL_b16edc6321384038aec3c06faa5d5889",
            "value": " 1.34M/1.34M [00:00&lt;00:00, 14.0MB/s]"
          }
        },
        "f233bf96fb3f4a0c8475b23bbcf4b09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4078c832b243cdb885ed1dc5713cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe85e322bc74ee297a01fabc2942757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3daada782f16478b8b27bbd612d7b940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696699949c9b4e32ae92dbd45db0172b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57489af57ceb4874b34561e3b8d06122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b16edc6321384038aec3c06faa5d5889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c21b85ad6484b2a91ae238ded11bdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7ec809e228d4a679e195d9c031c1593",
              "IPY_MODEL_886d5aee9ee348ba9e3cd4fb78bf512b",
              "IPY_MODEL_b1ee4cc1f3784737981a120987132932"
            ],
            "layout": "IPY_MODEL_311eda22f4b0427a9a2c654fc6c77021"
          }
        },
        "e7ec809e228d4a679e195d9c031c1593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ab8a35924a94da4b62d01f112c5d2cf",
            "placeholder": "​",
            "style": "IPY_MODEL_d46ffc4ea91c464badc7f9593aedda29",
            "value": "config.json: 100%"
          }
        },
        "886d5aee9ee348ba9e3cd4fb78bf512b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db489a0e84754ebbae1319228e5faa01",
            "max": 1416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_718b1e05205e4669a0ddeb424fe9e23e",
            "value": 1416
          }
        },
        "b1ee4cc1f3784737981a120987132932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18462d5d43754844b8911c95de0a1f67",
            "placeholder": "​",
            "style": "IPY_MODEL_7626fdc1576c48d0a022b59b4ad9017d",
            "value": " 1.42k/1.42k [00:00&lt;00:00, 44.8kB/s]"
          }
        },
        "311eda22f4b0427a9a2c654fc6c77021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ab8a35924a94da4b62d01f112c5d2cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d46ffc4ea91c464badc7f9593aedda29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db489a0e84754ebbae1319228e5faa01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718b1e05205e4669a0ddeb424fe9e23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18462d5d43754844b8911c95de0a1f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7626fdc1576c48d0a022b59b4ad9017d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5bbde62e74d435b94f74c12544cdd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4e74f579726485e973b065ab9af4183",
              "IPY_MODEL_7e8ce644c6584ccba81715cff1bb05e3",
              "IPY_MODEL_1aab2d6b84da4448ba3689e13bf4fb9a"
            ],
            "layout": "IPY_MODEL_9fcb9e3facfb4dc1a01e1707487e4783"
          }
        },
        "a4e74f579726485e973b065ab9af4183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1cf9d261fa4809873795e2b9ecddd6",
            "placeholder": "​",
            "style": "IPY_MODEL_ce0db6d8a522414caab25e054045b88f",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "7e8ce644c6584ccba81715cff1bb05e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ac734c1448240bcaf2fa54dea65ab0c",
            "max": 300827685,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_043d91cd18f34986bf7ab0ae3d44ba6a",
            "value": 300827685
          }
        },
        "1aab2d6b84da4448ba3689e13bf4fb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ca83a566b94e5f96ad7eef77787358",
            "placeholder": "​",
            "style": "IPY_MODEL_0ed4eed9349f4fc88ecf0140516711b1",
            "value": " 301M/301M [00:04&lt;00:00, 26.4MB/s]"
          }
        },
        "9fcb9e3facfb4dc1a01e1707487e4783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf1cf9d261fa4809873795e2b9ecddd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce0db6d8a522414caab25e054045b88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac734c1448240bcaf2fa54dea65ab0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043d91cd18f34986bf7ab0ae3d44ba6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01ca83a566b94e5f96ad7eef77787358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed4eed9349f4fc88ecf0140516711b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b71307827047bea0879b2f87c535f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5d196d39286478ba319f4d7769e4921",
              "IPY_MODEL_7e2e7d6729324f25a814214f2bb60d28",
              "IPY_MODEL_b13684f2d34d4371805377d4698c8c36"
            ],
            "layout": "IPY_MODEL_d470c4248a8b41e3bebf9f85d9c3b21e"
          }
        },
        "d5d196d39286478ba319f4d7769e4921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_242b006a3fbe4ac69234f0315be77a56",
            "placeholder": "​",
            "style": "IPY_MODEL_d31de5cffe26477cafcf65d29edbf797",
            "value": "generation_config.json: 100%"
          }
        },
        "7e2e7d6729324f25a814214f2bb60d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da83500038645f98bcc5db02f14b932",
            "max": 293,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48e2d52c1c514cb2b3976a3a23b6ca8c",
            "value": 293
          }
        },
        "b13684f2d34d4371805377d4698c8c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_745ee10439254bf89b90163c408fc5ce",
            "placeholder": "​",
            "style": "IPY_MODEL_ea8cfd4b592f494997032a607d203dda",
            "value": " 293/293 [00:00&lt;00:00, 13.4kB/s]"
          }
        },
        "d470c4248a8b41e3bebf9f85d9c3b21e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "242b006a3fbe4ac69234f0315be77a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d31de5cffe26477cafcf65d29edbf797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5da83500038645f98bcc5db02f14b932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e2d52c1c514cb2b3976a3a23b6ca8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "745ee10439254bf89b90163c408fc5ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8cfd4b592f494997032a607d203dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
